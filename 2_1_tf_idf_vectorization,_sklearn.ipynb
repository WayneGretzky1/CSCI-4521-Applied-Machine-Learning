{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WayneGretzky1/CSCI-4521-Applied-Machine-Learning/blob/main/2_1_tf_idf_vectorization%2C_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple tf-idf implementation:"
      ],
      "metadata": {
        "id": "knrJAx-PSAvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtL4-UajRvu6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def tfidf(term, doc, doc_list):\n",
        "  docs_with_term = [doc for doc in doc_list if term in doc]\n",
        "  tf = doc.count(term) #term count\n",
        "  df = len(docs_with_term)/len(doc_list) #document frequency\n",
        "  idf = math.log(1/df)\n",
        "  return tf * idf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"the mouse\", \"the small cat\", \"the cheese\", \"the big cat\"]"
      ],
      "metadata": {
        "id": "5PfCjzZ6XqSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: calculate the importance of each word in a document\n",
        "tfidf(\"cheese\", \"the small cat\", documents)"
      ],
      "metadata": {
        "id": "zBwOLaztXu6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2857da9c-7b54-493b-b1a6-175dfefb58d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getUniqueVocab(documents):\n",
        "  #Find unique vocab words\n",
        "  vocab = []\n",
        "  for doc in documents:\n",
        "    vocab.extend(doc.split())\n",
        "  vocab = set(vocab)\n",
        "  return vocab\n",
        "\n",
        "def vectorize(phrase, documents):\n",
        "  #Find unique vocab words\n",
        "  vocab = getUniqueVocab(documents)\n",
        "\n",
        "  #Build vector representation\n",
        "  result = []\n",
        "  for word in vocab:\n",
        "    result.append(tfidf(word, phrase, documents))\n",
        "  return result"
      ],
      "metadata": {
        "id": "bzcCa2ChI2fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: lets vectorize some documents!\n",
        "print(getUniqueVocab(documents))\n",
        "print(vectorize(\"the small cat\", documents))"
      ],
      "metadata": {
        "id": "qJpm_m8YZir-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1619ece-20cd-40f1-cfef-2ac147f81095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cheese', 'small', 'big', 'the', 'mouse', 'cat'}\n",
            "[0.0, 1.3862943611198906, 0.0, 0.0, 0.0, 0.6931471805599453]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## TF-IDF with SK-Learn\n",
        "\n",
        "This is a pretty cude approach to tf-idf. In practice we often want to smooth out the tf-idf computation, and include extra normalization terms.\n",
        "\n",
        "There are many corner cases to consider, and variations on how we can compute the `tf` term and the `idf` term.\n",
        "\n",
        "Its often useful to use a library to compute tf-idf, and one is provided in `SK Learn`"
      ],
      "metadata": {
        "id": "egPqe9z8SFbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "IZ3jr_oDTGko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create vectorizer and fit_transform our documents (training set)\n",
        "vectorizer = TfidfVectorizer(min_df=1)"
      ],
      "metadata": {
        "id": "jvFwJvs5byu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_docs = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "lX1z-mb-N8LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "6rPjlWrhOTiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e08cdba-0922-4dd5-ff8a-d21074470e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['big', 'cat', 'cheese', 'mouse', 'small', 'the'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[print(train_vec.toarray()) for train_vec in vectorized_docs]"
      ],
      "metadata": {
        "id": "gLdg_XkwOcmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8e9193-36bc-456e-f728-0ed6544a2350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.88654763 0.         0.46263733]]\n",
            "[[0.         0.5728925  0.         0.         0.72664149 0.37919167]]\n",
            "[[0.         0.         0.88654763 0.         0.         0.46263733]]\n",
            "[[0.72664149 0.5728925  0.         0.         0.         0.37919167]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that \"the\" (word #5) has a non-zero value even though it occurs in every document. \"The\" is still the smallest values (around 0.4), and \"cheese\"/\"mouse\" still the largest value (0.88).\n",
        "\n",
        "**TODO: ANSWER ON CHIME IN** For which examples does SK-Learn's TF-IDF formula give \"the\" the most weight? Why is that? Any advantages to this method?"
      ],
      "metadata": {
        "id": "zSKuQxxnU2Uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "actual sklearn formulat : tfidf = log(N/df) * ln(1+M/T) where N is the number of occurrences of the word in the corpus, Df is the total number of documents in the collection, M is the number of times the word occurs in this document, and T is the total number of words in this particular document."
      ],
      "metadata": {
        "id": "DddBkh1Tz0U7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `vectorizer.transform` along with `toarray()` to get the vectorized result of a new document:"
      ],
      "metadata": {
        "id": "jDc34rLQXb8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: print the vector version of \"the big cheese\"\n",
        "print(vectorizer.transform([\"the big cheese\"]).toarray())"
      ],
      "metadata": {
        "id": "vGrOmpsoVexJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4283f47f-8835-42ce-c6ba-ba1365f48cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.66338461 0.         0.66338461 0.         0.         0.34618161]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## Your turn:\n",
        "\n",
        "1. First, can you compute the distance between two vectors. My examples give the squared Euclidean distance, but you can use other distance terms (e.g. cosine distance).\n",
        "\n",
        "Here is a stub of `my_dist` to get you started"
      ],
      "metadata": {
        "id": "VSxIYoNhb12G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def my_dist(v1, v2):\n",
        "  return 0 # TODO: Update this to be the distance between v1 and v2"
      ],
      "metadata": {
        "id": "25V0ci0zi8Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_dist(np.array([3]),np.array([4])))          #Squared L2 Dist is 1\n",
        "print(my_dist(np.array([1,1]),np.array([2,2])))      #Squared L2 Dist is 2\n",
        "print(my_dist(np.array([1,2,3]),np.array([1,-1,7]))) #Squared L2 Dist is 25"
      ],
      "metadata": {
        "id": "xmucfoDPcFMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you above distance function works, you can now take the distance between every vector and an new document using code like this:"
      ],
      "metadata": {
        "id": "KTM39fCncsWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_post_vec = vectorizer.transform([\"the big cheese\"])\n",
        "[my_dist(new_post_vec.toarray(),train_vec.toarray()) for train_vec in vectorized_docs] #I get [1.7, 1.7, 0.5, 0.7], your results may differ"
      ],
      "metadata": {
        "id": "gGrrK1e7ZLiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now write a function called `findClosest` which takes as input a string, and returns a string of the document in `documents` which has the closest feature vector.\n",
        "\n",
        "Again, we'll give you a stub of `findClosest` to help get you started:"
      ],
      "metadata": {
        "id": "5xWTjbL1daCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findClosest(promt):\n",
        "  closest_id = 0  # TODO: Fix this to be the actual closest index\n",
        "  return documents[closest_id]"
      ],
      "metadata": {
        "id": "gHMo3t6Ui6Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findClosest(\"the big cheese\") #Should probably return \"the cheese\""
      ],
      "metadata": {
        "id": "7MSJyWj7YccJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Thought Experiment - **ANSWER ON CHIME IN**\n",
        " - What would you want to happen when the input is `\"a cheesy slice of pizza\"`\n",
        " - What happens in reality?\n",
        " - How might we fix this?"
      ],
      "metadata": {
        "id": "xtsC8pt6gNJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "findClosest(\"a cheesy slice of pizza\") #Should this stil return \"the cheese\"?"
      ],
      "metadata": {
        "id": "stSuIFw7gWJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}