{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WayneGretzky1/CSCI-4521-Applied-Machine-Learning/blob/main/3_3_regression_with_advertising_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and Dataset loading"
      ],
      "metadata": {
        "id": "u0GDzRSuw6nQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSsdYo9laZS1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)  #Removes printing in scientific notation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_pd = pd.read_csv(\"https://raw.githubusercontent.com/be-prado/csci4521/refs/heads/main/Advertising.csv\")"
      ],
      "metadata": {
        "id": "yM09iaxkp37d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_pd.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgd_BmYvqKs-",
        "outputId": "97bb3147-95ae-4f37-92f7-734fd7ca88b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0     TV  radio  newspaper  sales\n",
            "0           1  230.1   37.8       69.2   22.1\n",
            "1           2   44.5   39.3       45.1   10.4\n",
            "2           3   17.2   45.9       69.3    9.3\n",
            "3           4  151.5   41.3       58.5   18.5\n",
            "4           5  180.8   10.8       58.4   12.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting data (no normalization)"
      ],
      "metadata": {
        "id": "SYF6okjAxEGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create tensors for the TV budget data and the sales data\n"
      ],
      "metadata": {
        "id": "vL7zJGyYQi8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a scatter plot of them\n"
      ],
      "metadata": {
        "id": "s-w_rrygQsBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets implement gradient descent!"
      ],
      "metadata": {
        "id": "bvT71t21xaJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rate\n",
        "lr = 0.001 #Larger lr goes faster, but what about too large an lr?\n",
        "\n",
        "# Initialize parameters (initializing them small usually helps)\n",
        "# making sure to tell pytorch these are the variables we will be taking the gradient over\n",
        "m = torch.Tensor([0.1]).float()\n",
        "m.requires_grad = True\n",
        "b = torch.Tensor([0.1]).float()\n",
        "b.requires_grad = True\n",
        "\n",
        "# Gradient descent update loop\n",
        "for epoch in range(2000):\n",
        "  # forward pass\n",
        "  y_pred = m*x_pt+b\n",
        "  # compute loss\n",
        "  loss_tensor = (y_pred - y_pt)**2\n",
        "  loss = loss_tensor.mean()\n",
        "  # compute the gradient of our loss function\n",
        "  loss.backward()\n",
        "  with torch.no_grad(): # don't need to include this computation in our gradient\n",
        "    # update our parameters by using the learning rate to define step size\n",
        "    m -= m.grad*lr\n",
        "    b -= b.grad*lr\n",
        "    # reset our gradients\n",
        "    m.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "  if epoch%200==0:\n",
        "    print(\"m=\",m.item(),\"b=\",b.item(),\"loss=\",loss.item())"
      ],
      "metadata": {
        "id": "aPkIGiqGqfGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops, this breaks. Even with a small learning rate! (Try it.) The issue is that we are not normalizating our data. Gradient descent does much better with normalized data input."
      ],
      "metadata": {
        "id": "0jzqVyEHY69U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line fitting (with normalization)"
      ],
      "metadata": {
        "id": "MnHaCuwqxKeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the scatterplot above, we can see that TV advertising spending goes up to about 300, and the sales go up to about 30.\n",
        "\n",
        "So, to get our results to have a magnitude of around 1, we can do a very simple \"Order of magnitude\" normalization where we divide each term by a nice round number near the maximum value."
      ],
      "metadata": {
        "id": "2PoTfMpJxQsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: scale your TV budget tensor and your sales tensor so the values around a magnitude of 1\n"
      ],
      "metadata": {
        "id": "O7YQkl5eKGkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: copy and paste the same gradient descent code we used above but make sure to use our normalized tensors this time\n",
        "# make sure to use a small learning rate!\n",
        "\n"
      ],
      "metadata": {
        "id": "zWFMCjsfSPbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully you see that's much better! We can now see that we have an estimate for the slope $m$ and the y-intercept $b$ that has a relatively low loss.\n",
        "\n",
        "We can recompute the loss, just to double-check:"
      ],
      "metadata": {
        "id": "77BTAEWzZxlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: compute total mean squared error with the parameters our training loop ended with\n"
      ],
      "metadata": {
        "id": "zhGtcgP953mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the line we learned to see if it fits the data well."
      ],
      "metadata": {
        "id": "sDBGv_72aNEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=x_pt,y=y_pt) # make sure these variables match your normalized tensors\n",
        "y_intercept =  b.detach().numpy()\n",
        "slope =  m.detach().numpy()\n",
        "plt.plot([0,1],[y_intercept,slope+y_intercept], color=\"red\")"
      ],
      "metadata": {
        "id": "SiJMR0Q07K_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Code"
      ],
      "metadata": {
        "id": "Bc0_ern-xzLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code works well, but wasn't very structured. Let's fix that.\n",
        "\n",
        "First, we can put the linear model in its own function and make sure the parameters are stored in an array or tensor to structure the code better:"
      ],
      "metadata": {
        "id": "rYfpKO0Ax1c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearModel(params, inputs):\n",
        "  y_pred = params[0]*inputs + params[1]\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "_L5CRywGJUNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also encapsulate the gradient descent into a function."
      ],
      "metadata": {
        "id": "lK09fCB5yEua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradDec(model, n_params, x, y, lr=0.01, n_epochs=2000, print_rate=200):\n",
        "\n",
        "  params = 0.1*torch.rand(n_params).float() #Random inital paramaters\n",
        "  params.requires_grad = True\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    y_pred = model(params, x)\n",
        "    loss_tenor = (y_pred - y)**2\n",
        "    loss = loss_tenor.mean()\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      params -= lr*params.grad\n",
        "      params.grad.zero_()\n",
        "    if epoch%print_rate==0:\n",
        "      print(\"epoch:\",epoch,\"loss=\",loss.item())\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "dwO_3U53L43h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train everything in one line of code:"
      ],
      "metadata": {
        "id": "qYq-ghOOby-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use our new helper functions to redo the same analysis as above\n"
      ],
      "metadata": {
        "id": "Mpr7L4TuTAwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "cD0H2u_fND4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Linear Regression"
      ],
      "metadata": {
        "id": "KHBm0SO2kk6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through the same exercise again, but this time with multiple linear regression where we try to predict the model output as a linear combination of several input features."
      ],
      "metadata": {
        "id": "38Kf7dyqycMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"https://raw.githubusercontent.com/be-prado/csci4521/refs/heads/main/Advertising.csv\")"
      ],
      "metadata": {
        "id": "H2g6rC9fNJ_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corelation"
      ],
      "metadata": {
        "id": "tqHwVY6tyh6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building the model, let's first visualize which variables are correlated with each other, and which variables are correlated with the output (sales)."
      ],
      "metadata": {
        "id": "mbcf9hDqcFw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = data_df[[\"TV\",\"radio\",\"newspaper\",\"sales\"]].corr()\n",
        "sns.heatmap(corr, cmap = \"coolwarm\") #Try: RdBu_r vs. coolwarm"
      ],
      "metadata": {
        "id": "zkqAkuEFUZKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can already see that TV and radio are likely more useful than newspaper advertising (strong positive correlations)."
      ],
      "metadata": {
        "id": "bchNYqcByoz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data into Tensors\n",
        "\n",
        "We need to move the dataset we care about from Pandas dataframes into PyTorch tensors."
      ],
      "metadata": {
        "id": "e3h8XIw-cZ9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features=[\"TV\",\"radio\",\"newspaper\"]\n",
        "# TODO: load the values from \"TV\",\"radio\",\"newspaper\" into x and values from \"sales\" into y\n"
      ],
      "metadata": {
        "id": "NSfnRXJjNRfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single row in the input feature tensor *should* now be a vector with 3 values (TV, radio, and newspaper spending):"
      ],
      "metadata": {
        "id": "smn_HInVcq1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: print out a single row of x\n"
      ],
      "metadata": {
        "id": "1-vblO7nO6Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using the colon symbol `:` when accessing an array, we can pull out an entire column. Print out the TV advertising spending (column 0)."
      ],
      "metadata": {
        "id": "NnpxsBixc1Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: print out a single column of x\n"
      ],
      "metadata": {
        "id": "VsenazW5O7Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the tensors"
      ],
      "metadata": {
        "id": "BxgD9u1uy1qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw above, we need to normalize the tensors to get good results. Above, we did a simple order of magnitude normalization. Here, I'll normalize based on the mean of the data. (There are lots of ways to normalize data, and there is no best method. Different ideas can make sense for different datasets. Just make sure the numbers all end up with magnitudes around 1.)"
      ],
      "metadata": {
        "id": "6D21ssT0dGqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch tensors have a `.mean()` method to get the mean of the tensor. We want the mean along the columns (e.g., a separate mean for each feature), so we need to pass in `dim=0` to the mean function so it knows which direction to take the mean over."
      ],
      "metadata": {
        "id": "yL1MZ6_odeXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_mean = x.mean(dim=0)\n",
        "x_mean"
      ],
      "metadata": {
        "id": "RHNDXMHXPKCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can shift and divide by the mean for both the inputs x and outputs y. This is the same z-score normalization we've used before. This time with tensors."
      ],
      "metadata": {
        "id": "USBnPl7NdvR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_norm = (x-x_mean)/x_mean\n",
        "y_mean = y.mean(dim=0)\n",
        "y_norm = (y-y_mean)/y_mean"
      ],
      "metadata": {
        "id": "EACjXVyEPaUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `multiLinearModel` predicts the output as a scaled sum of all three input features (plus a constant shift):"
      ],
      "metadata": {
        "id": "MBK1sXWld0GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiLinearModel(params, inputs):\n",
        "  y_pred = params[0]*inputs[:,0] + params[1]*inputs[:,1] + params[2]*inputs[:,2] + params[3]\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "jcryJLqgNm6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our new model and structured code from above, we can train everything in one line."
      ],
      "metadata": {
        "id": "5l0CeanJeHWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use our new multilinear model helper function to train on our data again\n"
      ],
      "metadata": {
        "id": "AQsrLYjzOFrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops, the `newspaper` term is almost 0. We may want to discard it from our model and train again to reduce the risk of overfitting!"
      ],
      "metadata": {
        "id": "Vf_BWhd1zNCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-linear regression with interaction terms"
      ],
      "metadata": {
        "id": "nD7apFtnzAq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we may want to try a model where we consider interaction effects between the terms.\n",
        "\n",
        "In the model below we remove the term relating to newspaper advertising (see above). But we add a new term of `TV * Radio` ... this captures the intreaction between the type two avertisement. You can explore different interaction terms but the correlation heatmap can help."
      ],
      "metadata": {
        "id": "MDKKUEk4eVKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiLinearModel_withInteraction(params, inputs):\n",
        "  y_pred = params[0]*inputs[:,0] + params[1]*inputs[:,1] + params[2]*(inputs[:,0]*inputs[:,1]) + params[3]\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "4HaFOeVLbx6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use our new multilinear with interaction model helper function to train on our data again\n"
      ],
      "metadata": {
        "id": "bxEy3m2zcCAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the loss of the model with this interaction term is noticeably better than the previous model."
      ],
      "metadata": {
        "id": "dtMIaElFPaHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a gut-check, we can apply the model to the first five items:"
      ],
      "metadata": {
        "id": "we3FX3tdzGfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item  in range (5):\n",
        "  print(0.3581*x_norm[item][0] + 0.2699*x_norm[item][1]+0.1643*x_norm[item][0]*x_norm[item][1]-0.0027)"
      ],
      "metadata": {
        "id": "d9UUQcnqgSUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And checking our prediction:"
      ],
      "metadata": {
        "id": "hmqcaYJ0zIrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item  in range (5):\n",
        "  print(y_norm[item])"
      ],
      "metadata": {
        "id": "4N_UcUPEgkcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a bad match! Though for a real gut-check we would need to undo the normalization so we can report these values in the original units.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWqhXm3KO34D"
      }
    }
  ]
}