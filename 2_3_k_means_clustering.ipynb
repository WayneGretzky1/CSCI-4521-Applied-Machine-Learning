{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WayneGretzky1/CSCI-4521-Applied-Machine-Learning/blob/main/2_3_k_means_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "ZLmtACfX-MO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ha1gpkaZdHu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24897fd2-fbd2-41d7-85f1-86e03e3178ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-01 20:04:49--  https://raw.githubusercontent.com/be-prado/csci4521/refs/heads/main/20news-bydate.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14464277 (14M) [application/octet-stream]\n",
            "Saving to: ‘20news-bydate.tar.gz’\n",
            "\n",
            "20news-bydate.tar.g 100%[===================>]  13.79M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-01 20:04:49 (122 MB/s) - ‘20news-bydate.tar.gz’ saved [14464277/14464277]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/be-prado/csci4521/refs/heads/main/20news-bydate.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf 20news-bydate.tar.gz"
      ],
      "metadata": {
        "id": "3MgHxHdqdINR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "ZY-ssHOypWNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = \"/content/20news-bydate-train/\"\n",
        "catigories = [\"comp.graphics\",\"comp.sys.mac.hardware\",\"talk.politics.misc\",\"sci.space\",\"misc.forsale\"]"
      ],
      "metadata": {
        "id": "lgMrvE_ypXxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = []\n",
        "labels = []\n",
        "for c in catigories:\n",
        "  posts += [open(os.path.join(DIR+c, f), encoding=\"latin-1\").read() for f in os.listdir(DIR+c)]\n",
        "  labels += [c for f in os.listdir(DIR+c)]\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "UOcsUUXmpZRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize documents"
      ],
      "metadata": {
        "id": "2RRJfRmI-RYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A TF-IDF Vectorizer with Stemming\n",
        "import nltk.stem\n",
        "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "   def build_analyzer(self):\n",
        "     # analyzer callable from TfidfVectorizer that does tokenization\n",
        "     analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
        "     # new analyzer that performs stemming on the tokenization from the analyzer above\n",
        "     return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
        "# vectorized tf-idf matrix where each row represents a document and each\n",
        "# column represents a feature i.e. tf-idf score for each term\n",
        "X_train = vectorizer.fit_transform(posts)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "GhWyFvUspbNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40495b5-c2c9-4312-e17e-1d9e079ec390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2805, 31285)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering"
      ],
      "metadata": {
        "id": "idh8ACv6-akz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement k-means clustering using the sklearn\n",
        "num_clusters = 20\n",
        "km = KMeans(n_clusters = num_clusters, init = 'random', n_init = 10, verbose = 1)"
      ],
      "metadata": {
        "id": "UVogV9AopfdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.labels_ #The labels"
      ],
      "metadata": {
        "id": "UHs9MqP3pzSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.labels_.shape"
      ],
      "metadata": {
        "id": "20-Ws2lDp3tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.cluster_centers_ #The actual cluster centers"
      ],
      "metadata": {
        "id": "oxCwhDIgp5uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the model"
      ],
      "metadata": {
        "id": "LH5pRSj4ABQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: vectorize the sentence \"Used Car for Sale!\"\n"
      ],
      "metadata": {
        "id": "Sp0zN5eip-CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: predict the cluster for the sentence above\n"
      ],
      "metadata": {
        "id": "1w4x0zgzqFv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km.cluster_centers_[new_post_cluster] #We can manualy compare feature to cluster centers"
      ],
      "metadata": {
        "id": "hZM6CCRrf6ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which posts are also in my cluster?"
      ],
      "metadata": {
        "id": "hW5TEj2J_I9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_neighbor_indices = (km.labels_==new_post_cluster).nonzero()[0]\n",
        "print(cluster_neighbor_indices[0:10]) #first 10 cluster indicies"
      ],
      "metadata": {
        "id": "L1Vm8GxoqIPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the labels of the posts in the same cluster:"
      ],
      "metadata": {
        "id": "GztDipp4_WuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(labels[cluster_neighbor_indices],10) #print the label for 10 random indices"
      ],
      "metadata": {
        "id": "tc_e2zTL-3aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally, there should be a clear most common topic within this cluster, that well describes the test prompt.\n",
        "\n",
        "We can also sort, and find the k-nearest neighbors within this cluster:"
      ],
      "metadata": {
        "id": "ZstGyiXe_i2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_neighbors = []\n",
        "# for each post in my cluster\n",
        "for i in cluster_neighbor_indices:\n",
        "  # find the distance between new post and posts in the cluster\n",
        "  dist = np.linalg.norm((new_post_vec - X_train[i]).toarray())\n",
        "  # save the distance, the post, and the label into the list\n",
        "  cluster_neighbors.append((dist, posts[i], labels[i]))\n",
        "  # sort the list\n",
        "  # NOTE: The sorted() method sorts tuples by default, using the first item in each tuple (our distances!)\n",
        "  cluster_neighbors = sorted(cluster_neighbors)\n",
        "print(len(cluster_neighbors))"
      ],
      "metadata": {
        "id": "7r-SrSuzqRFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_neighbors[0]) #The nearest neighbor within the cluster"
      ],
      "metadata": {
        "id": "gNP8PK36qYJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_neighbors[len(cluster_neighbors)//2]) #The median neighbor within the cluster"
      ],
      "metadata": {
        "id": "7BcJrnJ5qgm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_neighbors[-1]) #The furthest away neighbor still in the cluster"
      ],
      "metadata": {
        "id": "yb48r0Peqkmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Clouds"
      ],
      "metadata": {
        "id": "AJ5ADyfygjQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "t-Yzb4rGfOsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = np.array(posts)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "ruRwQW7xfQYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each cluster...\n",
        "for cluster_id in range(0,num_clusters):\n",
        "  # grab all the posts in the cluster\n",
        "  matching_indices = (km.labels_== cluster_id)\n",
        "  # concatonate them into a single 'document'\n",
        "  all_text = \" \".join(txt for txt in posts[matching_indices.nonzero()[0]])\n",
        "  # make a word cloud of it\n",
        "  word_cloud1 = WordCloud(collocations = False, background_color = 'white',\n",
        "                          width = 2048, height = 1080).generate(all_text)\n",
        "  plt.imshow(word_cloud1, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  # print how many posts were in that cluster\n",
        "  print(\"cluster size: \",posts[matching_indices].size)\n",
        "  # print the topics of the first 10 posts in the cluster\n",
        "  print(\"-\"+\"\\n-\".join(txt for txt in labels[matching_indices][0:10]))"
      ],
      "metadata": {
        "id": "tcPSGrlsfRxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Cloud (TF-IDF)"
      ],
      "metadata": {
        "id": "FAwQvozKhptr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "XjY4H1zfhm5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unstemed_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "vecs = unstemed_vectorizer.fit_transform(posts)\n",
        "feature_names = unstemed_vectorizer.get_feature_names_out()\n",
        "dense = vecs.todense() # brings the vecs sparse matrix into regular matrix format\n",
        "tfidf_df = pd.DataFrame(dense, columns=feature_names)\n",
        "\n",
        "for cluster_id in range(0,num_clusters):\n",
        "  matching_indices = (km.labels_== cluster_id)\n",
        "  word_tfidf = tfidf_df[matching_indices].T.sum(axis=1)\n",
        "  word_cloud1 = WordCloud(collocations = False, background_color = 'white', max_words=60,\n",
        "                          width = 2048, height = 1080).generate_from_frequencies(word_tfidf)\n",
        "                          # instead of using frequencies, override with the tfidf val\n",
        "  plt.imshow(word_cloud1, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  print(\"cluster size: \",posts[matching_indices].size)\n",
        "  print(\"-\"+\"\\n-\".join(txt for txt in np.random.choice(labels[matching_indices],10)))"
      ],
      "metadata": {
        "id": "M27AACE6hAW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same as above but with stems (can be a bit harder to interpret)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "dense = X_train.todense() # brings the vecs sparse matrix into regular matrix format\n",
        "tfidf_df = pd.DataFrame(dense, columns=feature_names)\n",
        "\n",
        "for cluster_id in range(0,num_clusters):\n",
        "  matching_indices = (km.labels_== cluster_id)\n",
        "  word_tfidf = tfidf_df[matching_indices].T.sum(axis=1)\n",
        "  word_cloud1 = WordCloud(collocations = False, background_color = 'white', max_words=60,\n",
        "                          width = 2048, height = 1080).generate_from_frequencies(word_tfidf)\n",
        "                          # instead of using frequencies, override with the tfidf val\n",
        "  plt.imshow(word_cloud1, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "  print(\"cluster size: \",posts[matching_indices].size)\n",
        "  print(\"-\"+\"\\n-\".join(txt for txt in np.random.choice(labels[matching_indices],10)))"
      ],
      "metadata": {
        "id": "wVX_ygUjeuRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn!"
      ],
      "metadata": {
        "id": "rLB7m5h76UjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the cluster of your own sentence and print the word cloud of that cluster to see if your output seems sensible."
      ],
      "metadata": {
        "id": "gD08Rfb_6bAl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmQfmxJaj0D6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}